# -*- coding: utf-8 -*-
"""Added code from other source of Social media sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vlug6qpLSDiL33qjXzegYEtkgkdPSMjF
"""
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Example model loading and fitting
def load_model():
    model = Pipeline ([
        ('vect', CountVectorizer()),
        ('clf', MultinomialNB()),
    ])
    return model

def train_and_save_model():
    # Load dataset
    df = pd.read_csv('path/to/your/dataset.csv')
    X = df['text']
    y = df['label']

nltk.download('punkt')
nltk.download('stopwords')

#Load dataset
df = pd.read_csv(r'C:\Users\PC\Downloads\Sentiment model\sentimentdataset1.csv')
df.head()

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenize text
    tokens = word_tokenize(text)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove punctuation and stopwords
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

    return ' '.join(tokens)

# Apply preprocessing
df['Sentiment'] = df['Text'].apply(preprocess_text)
df.head()

tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(df['Sentiment']).toarray()
y = df['Sentiment']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

 # Initialize and fit model
    model = Pipeline ([
        ('vect', CountVectorizer()),
        ('clf', LogisticRegression())
    ])
    model.fit(X_train, y_train)
    
    # Save the model
    joblib.dump(model, 'sentiment_model.pkl')

def load_model():
    # Load the fitted model
    model = joblib.load('sentiment_model.pkl')
    return model

# Initialize the model and pipeline
model = Pipeline([
    ('vect', CountVectorizer()),
    ('clf', LogisticRegression())  # Make sure you use the correct model
])
    
# Fit the model with training data
model = LogisticRegression()
model.fit(X_train, y_train)
# Predict on the test set
y_pred = model.predict(X_test)

# prompt: Using dataframe df: generate a relation between positive, negative and neutral sentiment

# Calculate the counts for each sentiment category
positive_counts = df['Positive'].value_counts()
negative_counts = df['Negative'].value_counts()
neutral_counts = df['Neutral'].value_counts()

# Create a summary DataFrame
sentiment_relation = pd.DataFrame({
    'Positive': positive_counts,
    'Negative': negative_counts,
    'Neutral': neutral_counts
})

# Display the sentiment relationship
print(sentiment_relation)

# prompt: Using dataframe df: generate code to display positive, negative and neutral and the number

# Count the number of positive, negative, and neutral sentiments
positive_count = df['Positive'].count()
negative_count = df['Negative'].count()
neutral_count = df['Neutral'].count()

# Print the counts
print("Positive:", positive_count)
print("Negative:", negative_count)
print("Neutral:", neutral_count)

# prompt: Using dataframe df: comparison between likes and retweets

import pandas as pd
import matplotlib.pyplot as plt

# Create a scatter plot with likes on the x-axis and retweets on the y-axis
df.plot.scatter(x='Likes', y='Retweets')

# Add a title and axis labels
plt.title('Comparison Between Likes and Retweets')
plt.xlabel('Likes')
plt.ylabel('Retweets')

# Show the plot
plt.show()

# @title Retweets

from matplotlib import pyplot as plt
df['Retweets'].plot(kind='hist', bins=20, title='Retweets')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title User Engagement by Platform

import matplotlib.pyplot as plt

df_grouped = df.groupby('Platform').sum()

plt.figure(figsize=(10, 6))
plt.bar(df_grouped.index, df_grouped['Likes'], label='Likes')
plt.bar(df_grouped.index, df_grouped['Retweets'], bottom=df_grouped['Likes'], label='Retweets')

plt.xlabel('Platform')
plt.ylabel('Total Engagement')
plt.title('User Engagement by Platform')
_ = plt.legend()

# @title Likes

from matplotlib import pyplot as plt
df['Likes'].plot(kind='hist', bins=20, title='Likes')
plt.gca().spines[['top', 'right',]].set_visible(False)



# Calculate accuracy
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Logistic regression model's accuracy = {test_accuracy:.4f}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load your dataset (assuming 'X' is your features and 'y' is your target)
# Replace with your actual data loading code
X = pd.read_csv('sentimentdataset1.csv')
y = X.pop('Sentiment')  # Replace 'target_column' with your actual target column name

# Identify non-numeric columns
non_numeric_columns = X.select_dtypes(exclude=['number']).columns

# Drop or encode non-numeric columns before scaling
X = X.drop(columns=non_numeric_columns)  # Drop non-numeric columns

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Example of standardizing features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and fit a logistic regression model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Predict on the test set and evaluate accuracy
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search over
param_grid = {
    'C': [0.1, 1.0, 10.0],  # Regularization parameter
    'penalty': ['l1', 'l2']  # Penalty ('l1' for Lasso, 'l2' for Ridge)
}

# Initialize logistic regression model
model = LogisticRegression()

# Setup grid search cross-validation
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit grid search
grid_search.fit(X_train_scaled, y_train)

# Print best parameters and best score
print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))

# Evaluate on the test set with best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f'Test set accuracy with best model: {accuracy}')

y_pred = model.predict(X_test)

print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print(f'Classification Report: \n{classification_report(y_test, y_pred, zero_division=1)}')

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Load data
data = load_iris()
X, y = data.data, data.target

# Initialize the model
model = RandomForestClassifier()

# Perform k-fold cross-validation
scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation

print("Cross-Validation Scores:", scores)
print("Mean Accuracy:", scores.mean())

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize the model
model = RandomForestClassifier()

# Initialize Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)

# Fit Grid Search
grid_search.fit(X, y)

print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a logistic regression
# logreg = LogisticRegression(C=1e9, solver='lbfgs', max_iter=1000)
# logreg.fit(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a random forest classifier
# forest = RandomForestClassifier(n_estimators=50)
# forest.fit(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a linear support vector classifier (LinearSVC)
# svc = LinearSVC()
# svc.fit(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a multinomial naive bayes classifier (MultinomialNB)
# bayes = MultinomialNB()
# bayes.fit(X, y)

# Create some test data

pd.set_option("display.max_colwidth", 200)

unknown = pd.DataFrame({'content': [
    "I love love love love this kitten",
    "I hate hate hate hate this keyboard",
    "I'm not sure how I feel about toast",
    "Did you see the baseball game yesterday?",
    "The package was delivered late and the contents were broken",
    "Trashy television shows are some of my favorites",
    "I'm seeing a Kubrick film tomorrow, I hear not so great things about it.",
    "I find chirping birds irritating, but I know I'm not the only one",
]})
unknown

from sklearn.feature_extraction.text import CountVectorizer  # Import CountVectorizer

# Assuming 'unknown' is your DataFrame with text data in a 'content' column
vectorizer = CountVectorizer()  # Initialize CountVectorizer
X = vectorizer.fit_transform(unknown['content'])  # Fit and transform your text data

print(vectorizer.get_feature_names_out())  # Use get_feature_names_out() for scikit-learn versions >= 1.0

# prompt: Using dataframe df: I want to display the  positive words and negative words and count the positive and negative words

# Display positive and negative words
print("Positive Words:")
print(df['Positive'].unique())
print("Neutral Words:")
print(df['Neutral'].unique())
print("\nNegative Words:")
print(df['Negative'].unique())

# Count positive and negative words
positive_count = df['Positive'].nunique()
negative_count = df['Neutral'].nunique()
negative_count = df['Negative'].nunique()

print("\nCount of Positive Words:", positive_count)
print("\nCount of Neutral Words:", neutral_count)
print("Count of Negative Words:", negative_count)